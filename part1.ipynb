{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "a7679431",
      "metadata": {
        "id": "a7679431"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "642d5289",
      "metadata": {
        "id": "642d5289"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "import torch\n",
        "import nltk\n",
        "import string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aab18b2f",
      "metadata": {
        "id": "aab18b2f",
        "outputId": "6f3b6d64-4517-4a30-bc36-8905aed286f2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True\n",
            "NVIDIA GeForce RTX 4050 Laptop GPU\n"
          ]
        }
      ],
      "source": [
        "print(torch.cuda.is_available())\n",
        "print(torch.cuda.get_device_name(0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e2b9207",
      "metadata": {
        "id": "9e2b9207",
        "outputId": "be49bea9-199f-4fe4-9507-207366ce2b74"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\reda\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     C:\\Users\\reda\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "00552eee",
      "metadata": {
        "id": "00552eee"
      },
      "source": [
        "## Downloading dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f3e3a8d2",
      "metadata": {
        "id": "f3e3a8d2"
      },
      "outputs": [],
      "source": [
        "IMDB_Dataset = load_dataset(\"stanfordnlp/imdb\") #DatasetDict object"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d7bdddd",
      "metadata": {
        "id": "3d7bdddd"
      },
      "source": [
        "### Understanding the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11b626c8",
      "metadata": {
        "id": "11b626c8",
        "outputId": "44aa3306-6da9-4cdf-b67f-15353c789e91"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['text', 'label'],\n",
              "        num_rows: 25000\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['text', 'label'],\n",
              "        num_rows: 25000\n",
              "    })\n",
              "    unsupervised: Dataset({\n",
              "        features: ['text', 'label'],\n",
              "        num_rows: 50000\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "IMDB_Dataset # contains 3 splits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "27c44413",
      "metadata": {
        "id": "27c44413",
        "outputId": "e97687df-efe7-41ff-fa5b-cb521c8d0d3f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['text', 'label'],\n",
              "    num_rows: 50000\n",
              "})"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "IMDB_Dataset['unsupervised']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f499e831",
      "metadata": {
        "id": "f499e831",
        "outputId": "32f5555b-8653-47ab-f787-13725b6d698f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'text': Value('string'), 'label': ClassLabel(names=['neg', 'pos'])}"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "IMDB_Dataset['unsupervised'].features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd100cab",
      "metadata": {
        "id": "fd100cab",
        "outputId": "b09c7e52-3b2b-4022-d278-2ed9a43c6d34"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "50000"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "IMDB_Dataset['unsupervised'].num_rows # number of \"documents\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "506cfdfe",
      "metadata": {
        "id": "506cfdfe",
        "outputId": "f6c98d77-d46b-4aae-c996-352548a31a9d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Row 0:\n",
            "{'text': 'This is just a precious little diamond. The play, the script are excellent. I cant compare this movie with anything else, maybe except the movie \"Leon\" wonderfully played by Jean Reno and Natalie Portman. But... What can I say about this one? This is the best movie Anne Parillaud has ever played in (See please \"Frankie Starlight\", she\\'s speaking English there) to see what I mean. The story of young punk girl Nikita, taken into the depraved world of the secret government forces has been exceptionally over used by Americans. Never mind the \"Point of no return\" and especially the \"La femme Nikita\" TV series. They cannot compare the original believe me! Trash these videos. Buy this one, do not rent it, BUY it. BTW beware of the subtitles of the LA company which \"translate\" the US release. What a disgrace! If you cant understand French, get a dubbed version. But you\\'ll regret later :)', 'label': -1}\n",
            "Text ALONE: This is just a precious little diamond. The play, the script are excellent. I cant compare this movie with anything else, maybe except the movie \"Leon\" wonderfully played by Jean Reno and Natalie Portman. But... What can I say about this one? This is the best movie Anne Parillaud has ever played in (See please \"Frankie Starlight\", she's speaking English there) to see what I mean. The story of young punk girl Nikita, taken into the depraved world of the secret government forces has been exceptionally over used by Americans. Never mind the \"Point of no return\" and especially the \"La femme Nikita\" TV series. They cannot compare the original believe me! Trash these videos. Buy this one, do not rent it, BUY it. BTW beware of the subtitles of the LA company which \"translate\" the US release. What a disgrace! If you cant understand French, get a dubbed version. But you'll regret later :)\n",
            "Label ALONE: -1\n",
            "----------------------------------------------------------------\n",
            "Row 1:\n",
            "{'text': 'When I say this is my favourite film of all time, that comment is not to be taken lightly. I probably watch far too many films than is healthy for me, and have loved quite a few of them. I first saw \"La Femme Nikita\" nearly ten years ago, and it still manages to be my absolute favourite. Why?<br /><br />This is more than an incredibly stylish and sexy thriller. Luc Besson\\'s great flair for impeccable direction, fashion, and appropriate usage of music makes this a very watchable film. But it is Anne Parillaud\\'s perfect rendering of a complex character who transforms from a heartless killer into a compassionate, vibrant young woman that makes this film beautiful. I can\\'t keep my eyes off of her when she is on screen.<br /><br />I have seen several of Luc Besson\\'s films including \"Subway\", \"The Professional\", and the irritating \"Fifth Element\", and \"Nikita\" is without a doubt, far superior to any of these. Although this film has tragic elements, it is ultimately extremely hopeful. It is the story of a person who is cruel and merciless, who ultimately comes to realize her own humanity and her own personal power. That, to me is extremely inspiring. If there is hope for Nikita, there is hope for all of us.', 'label': -1}\n",
            "Text ALONE: When I say this is my favourite film of all time, that comment is not to be taken lightly. I probably watch far too many films than is healthy for me, and have loved quite a few of them. I first saw \"La Femme Nikita\" nearly ten years ago, and it still manages to be my absolute favourite. Why?<br /><br />This is more than an incredibly stylish and sexy thriller. Luc Besson's great flair for impeccable direction, fashion, and appropriate usage of music makes this a very watchable film. But it is Anne Parillaud's perfect rendering of a complex character who transforms from a heartless killer into a compassionate, vibrant young woman that makes this film beautiful. I can't keep my eyes off of her when she is on screen.<br /><br />I have seen several of Luc Besson's films including \"Subway\", \"The Professional\", and the irritating \"Fifth Element\", and \"Nikita\" is without a doubt, far superior to any of these. Although this film has tragic elements, it is ultimately extremely hopeful. It is the story of a person who is cruel and merciless, who ultimately comes to realize her own humanity and her own personal power. That, to me is extremely inspiring. If there is hope for Nikita, there is hope for all of us.\n",
            "Label ALONE: -1\n",
            "----------------------------------------------------------------\n",
            "Row 2:\n",
            "{'text': 'I saw this movie because I am a huge fan of the TV series of the same name starring Roy Dupuis and Pet Wilson. The movie was really good and I saw how the TV show is based on the movie. A few episodes of the TV series came directly from the movie and their similarity was amazing. To keep things short, any fan of the movie has to watch the series and any fan of the series must see the original Nikita.', 'label': -1}\n",
            "Text ALONE: I saw this movie because I am a huge fan of the TV series of the same name starring Roy Dupuis and Pet Wilson. The movie was really good and I saw how the TV show is based on the movie. A few episodes of the TV series came directly from the movie and their similarity was amazing. To keep things short, any fan of the movie has to watch the series and any fan of the series must see the original Nikita.\n",
            "Label ALONE: -1\n",
            "----------------------------------------------------------------\n",
            "Row 3:\n",
            "{'text': \"Being that the only foreign films I usually like star a Japanese person in a rubber suit who crushes little tiny buildings and tanks, I had high hopes for this movie. I thought that this was a movie that wouldn't put me to sleep. WRONG! Starts off with a bang, okay, now she's in training, alright, she's an assassin, I'm still with you, oh, now she's having this moral dilemma and she can't decide if she loves her boyfriend or her controller, zzzzz.... Oh well, back to Gamera!\", 'label': -1}\n",
            "Text ALONE: Being that the only foreign films I usually like star a Japanese person in a rubber suit who crushes little tiny buildings and tanks, I had high hopes for this movie. I thought that this was a movie that wouldn't put me to sleep. WRONG! Starts off with a bang, okay, now she's in training, alright, she's an assassin, I'm still with you, oh, now she's having this moral dilemma and she can't decide if she loves her boyfriend or her controller, zzzzz.... Oh well, back to Gamera!\n",
            "Label ALONE: -1\n",
            "----------------------------------------------------------------\n",
            "Row 4:\n",
            "{'text': \"After seeing Point of No Return (a great movie) and being told that the original was better, I was certainly thrilled to see that one of the indie film channels was running La Femme Nikita. Then I saw the movie. Ouch! This was a major let-down.<br /><br />Nikita herself reminds me of Jar Jar Binks more than any other character I've seen recently. She comes across entirely as comic relief. The movie simply has nothing to recommend it besides the core concept of an evil, inhuman character paradoxically learning to be human while training as an assassin, and that concept failed miserably in Nikita due to the poor writing of the title role.\", 'label': -1}\n",
            "Text ALONE: After seeing Point of No Return (a great movie) and being told that the original was better, I was certainly thrilled to see that one of the indie film channels was running La Femme Nikita. Then I saw the movie. Ouch! This was a major let-down.<br /><br />Nikita herself reminds me of Jar Jar Binks more than any other character I've seen recently. She comes across entirely as comic relief. The movie simply has nothing to recommend it besides the core concept of an evil, inhuman character paradoxically learning to be human while training as an assassin, and that concept failed miserably in Nikita due to the poor writing of the title role.\n",
            "Label ALONE: -1\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "for i in range(5):\n",
        "    print(f\"Row {i}:\")\n",
        "    print(f\"{IMDB_Dataset['unsupervised'][i]}\")\n",
        "    print(f\"Text ALONE: {IMDB_Dataset['unsupervised']['text'][i]}\")\n",
        "    print(f\"Label ALONE: {IMDB_Dataset['unsupervised']['label'][i]}\")\n",
        "    print(\"----------------------------------------------------------------\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ca621a9",
      "metadata": {
        "id": "4ca621a9"
      },
      "outputs": [],
      "source": [
        "# for convenience\n",
        "dataset = IMDB_Dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_text = dataset['unsupervised']['text']\n",
        "unsupervised_texts = dataset_text"
      ],
      "metadata": {
        "id": "w6wOlfcm57eh"
      },
      "id": "w6wOlfcm57eh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "bf9a4e1a",
      "metadata": {
        "id": "bf9a4e1a"
      },
      "source": [
        "### Text only"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "593c6f00",
      "metadata": {
        "id": "593c6f00",
        "outputId": "42222ce3-041e-4bb2-e228-8bece22a58d2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Text 0\n",
            "This is just a precious little diamond. The play, the script are excellent. I cant compare this movie with anything else, maybe except the movie \"Leon\" wonderfully played by Jean Reno and Natalie Portman. But... What can I say about this one? This is the best movie Anne Parillaud has ever played in (See please \"Frankie Starlight\", she's speaking English there) to see what I mean. The story of young punk girl Nikita, taken into the depraved world of the secret government forces has been exceptionally over used by Americans. Never mind the \"Point of no return\" and especially the \"La femme Nikita\" TV series. They cannot compare the original believe me! Trash these videos. Buy this one, do not rent it, BUY it. BTW beware of the subtitles of the LA company which \"translate\" the US release. What a disgrace! If you cant understand French, get a dubbed version. But you'll regret later :)\n",
            "Text 1\n",
            "When I say this is my favourite film of all time, that comment is not to be taken lightly. I probably watch far too many films than is healthy for me, and have loved quite a few of them. I first saw \"La Femme Nikita\" nearly ten years ago, and it still manages to be my absolute favourite. Why?<br /><br />This is more than an incredibly stylish and sexy thriller. Luc Besson's great flair for impeccable direction, fashion, and appropriate usage of music makes this a very watchable film. But it is Anne Parillaud's perfect rendering of a complex character who transforms from a heartless killer into a compassionate, vibrant young woman that makes this film beautiful. I can't keep my eyes off of her when she is on screen.<br /><br />I have seen several of Luc Besson's films including \"Subway\", \"The Professional\", and the irritating \"Fifth Element\", and \"Nikita\" is without a doubt, far superior to any of these. Although this film has tragic elements, it is ultimately extremely hopeful. It is the story of a person who is cruel and merciless, who ultimately comes to realize her own humanity and her own personal power. That, to me is extremely inspiring. If there is hope for Nikita, there is hope for all of us.\n",
            "Text 2\n",
            "I saw this movie because I am a huge fan of the TV series of the same name starring Roy Dupuis and Pet Wilson. The movie was really good and I saw how the TV show is based on the movie. A few episodes of the TV series came directly from the movie and their similarity was amazing. To keep things short, any fan of the movie has to watch the series and any fan of the series must see the original Nikita.\n",
            "Text 3\n",
            "Being that the only foreign films I usually like star a Japanese person in a rubber suit who crushes little tiny buildings and tanks, I had high hopes for this movie. I thought that this was a movie that wouldn't put me to sleep. WRONG! Starts off with a bang, okay, now she's in training, alright, she's an assassin, I'm still with you, oh, now she's having this moral dilemma and she can't decide if she loves her boyfriend or her controller, zzzzz.... Oh well, back to Gamera!\n",
            "Text 4\n",
            "After seeing Point of No Return (a great movie) and being told that the original was better, I was certainly thrilled to see that one of the indie film channels was running La Femme Nikita. Then I saw the movie. Ouch! This was a major let-down.<br /><br />Nikita herself reminds me of Jar Jar Binks more than any other character I've seen recently. She comes across entirely as comic relief. The movie simply has nothing to recommend it besides the core concept of an evil, inhuman character paradoxically learning to be human while training as an assassin, and that concept failed miserably in Nikita due to the poor writing of the title role.\n"
          ]
        }
      ],
      "source": [
        "for i in range(5):\n",
        "    print(\"Text\",i)\n",
        "    print(dataset_text[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a4f32742",
      "metadata": {
        "id": "a4f32742",
        "outputId": "1693f6d0-9578-4edb-8e89-f0f0738663c6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "50000"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(dataset_text) # 50k strings"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7fd75cf0",
      "metadata": {
        "id": "7fd75cf0"
      },
      "source": [
        "# Actual Work"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c0fa64a5",
      "metadata": {
        "id": "c0fa64a5"
      },
      "source": [
        "## Step 1: Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3c8d2eb8",
      "metadata": {
        "id": "3c8d2eb8"
      },
      "source": [
        "### Different Methods of Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b7683845",
      "metadata": {
        "id": "b7683845",
        "outputId": "ceb3cbf1-a49a-41e9-b5f4-4853e39ef355"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Before tokenization: This is just a precious little diamond. The play, the script are excellent. I cant compare this movie with anything else, maybe except the movie \"Leon\" wonderfully played by Jean Reno and Natalie Portman. But... What can I say about this one? This is the best movie Anne Parillaud has ever played in (See please \"Frankie Starlight\", she's speaking English there) to see what I mean. The story of young punk girl Nikita, taken into the depraved world of the secret government forces has been exceptionally over used by Americans. Never mind the \"Point of no return\" and especially the \"La femme Nikita\" TV series. They cannot compare the original believe me! Trash these videos. Buy this one, do not rent it, BUY it. BTW beware of the subtitles of the LA company which \"translate\" the US release. What a disgrace! If you cant understand French, get a dubbed version. But you'll regret later :)\n"
          ]
        }
      ],
      "source": [
        "print(\"Before tokenization:\", dataset_text[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "503042f9",
      "metadata": {
        "id": "503042f9",
        "outputId": "524e13ca-a5ec-4568-cce9-c4c7d0e31678"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First few tokens using .split(): ['This', 'is', 'just', 'a', 'precious', 'little', 'diamond.', 'The', 'play,', 'the', 'script', 'are', 'excellent.', 'I', 'cant']\n"
          ]
        }
      ],
      "source": [
        "# OPTION 1\n",
        "# will not use this tokenization because it leaves out punctuations\n",
        "simple_tokens = dataset_text[0].split()\n",
        "print(\"First few tokens using .split():\",simple_tokens[:15])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0f3d1434",
      "metadata": {
        "id": "0f3d1434",
        "outputId": "3523ab5a-ab1e-43c5-ea60-c1bde174fcbc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First few tokens using nltk.tokenize.word_tokenize(): ['This', 'is', 'just', 'a', 'precious', 'little', 'diamond', '.', 'The', 'play', ',', 'the', 'script', 'are', 'excellent']\n"
          ]
        }
      ],
      "source": [
        "# OPTION 2\n",
        "# Better alternative using Natural Language Tool Kit\n",
        "# -> tokenizes into words and punctuation -- not actual tokens\n",
        "nltk_tokens = nltk.tokenize.word_tokenize(dataset_text[0])\n",
        "print(\"First few tokens using nltk.tokenize.word_tokenize():\", nltk_tokens[:15])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e65f4f4f",
      "metadata": {
        "id": "e65f4f4f",
        "outputId": "8b987963-d8d5-40b5-ed1c-be55e0e8deab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['this', 'is', 'just', 'a', 'precious', 'little', 'diamond', 'the', 'play', 'the', 'script', 'are', 'excellent', 'i', 'cant']\n"
          ]
        }
      ],
      "source": [
        "# testing normalization\n",
        "punctuation = set(string.punctuation)\n",
        "\n",
        "# 1. Lowercase the token\n",
        "# 2. Filter out tokens that are only punctuation\n",
        "\n",
        "test_normalized_tokens = [\n",
        "    token.lower() for token in nltk_tokens if token not in punctuation\n",
        "]\n",
        "\n",
        "print(test_normalized_tokens[:15])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f5ab48fc",
      "metadata": {
        "id": "f5ab48fc"
      },
      "outputs": [],
      "source": [
        "# OPTION 3: Advanced Tokenization --- maybe not?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ad8b416b",
      "metadata": {
        "id": "ad8b416b"
      },
      "source": [
        "### Tokenization and Text Normalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "37886f19",
      "metadata": {
        "id": "37886f19"
      },
      "outputs": [],
      "source": [
        "# I WILL USE OPTION 2: AT LEAST FOR NOW-- because it is the simplest and can be used to remove punctuation\n",
        "l = len(dataset_text)\n",
        "\n",
        "punctuation_to_filter = set(string.punctuation)\n",
        "# punctuation that slips through\n",
        "punctuation_to_filter.add(\"''\")\n",
        "punctuation_to_filter.add(\"``\")\n",
        "punctuation_to_filter.add(\"--\")\n",
        "punctuation_to_filter.add(\"'-\")\n",
        "punctuation_to_filter.add(\"'.\")\n",
        "\n",
        "normalized_tokens = [[] for _ in range(l)]\n",
        "doc_tokens = ['']*l\n",
        "\n",
        "for i in range(l):\n",
        "    doc_tokens[i] = nltk.tokenize.word_tokenize(dataset_text[i]) # tokenizing to words and punctuation\n",
        "    normalized_tokens[i] = [token.lower() for token in doc_tokens[i] if token not in punctuation_to_filter] # lower case and filter punctuation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f9ba38f",
      "metadata": {
        "id": "5f9ba38f",
        "outputId": "2de462fa-f2e0-4480-fb88-f38c5521a937"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['When', 'I', 'say', 'this', 'is', 'my', 'favourite', 'film', 'of', 'all', 'time', ',', 'that', 'comment', 'is', 'not', 'to', 'be', 'taken', 'lightly', '.', 'I', 'probably', 'watch', 'far', 'too', 'many', 'films', 'than', 'is', 'healthy', 'for', 'me', ',', 'and', 'have', 'loved', 'quite', 'a', 'few', 'of', 'them', '.', 'I', 'first', 'saw', '``', 'La', 'Femme', 'Nikita', \"''\", 'nearly', 'ten', 'years', 'ago', ',', 'and', 'it', 'still', 'manages', 'to', 'be', 'my', 'absolute', 'favourite', '.', 'Why', '?', '<', 'br', '/', '>', '<', 'br', '/', '>', 'This', 'is', 'more', 'than', 'an', 'incredibly', 'stylish', 'and', 'sexy', 'thriller', '.', 'Luc', 'Besson', \"'s\", 'great', 'flair', 'for', 'impeccable', 'direction', ',', 'fashion', ',', 'and', 'appropriate', 'usage', 'of', 'music', 'makes', 'this', 'a', 'very', 'watchable', 'film', '.', 'But', 'it', 'is', 'Anne', 'Parillaud', \"'s\", 'perfect', 'rendering', 'of', 'a', 'complex', 'character', 'who', 'transforms', 'from', 'a', 'heartless', 'killer', 'into', 'a', 'compassionate', ',', 'vibrant', 'young', 'woman', 'that', 'makes', 'this', 'film', 'beautiful', '.', 'I', 'ca', \"n't\", 'keep', 'my', 'eyes', 'off', 'of', 'her', 'when', 'she', 'is', 'on', 'screen.', '<', 'br', '/', '>', '<', 'br', '/', '>', 'I', 'have', 'seen', 'several', 'of', 'Luc', 'Besson', \"'s\", 'films', 'including', '``', 'Subway', \"''\", ',', '``', 'The', 'Professional', \"''\", ',', 'and', 'the', 'irritating', '``', 'Fifth', 'Element', \"''\", ',', 'and', '``', 'Nikita', \"''\", 'is', 'without', 'a', 'doubt', ',', 'far', 'superior', 'to', 'any', 'of', 'these', '.', 'Although', 'this', 'film', 'has', 'tragic', 'elements', ',', 'it', 'is', 'ultimately', 'extremely', 'hopeful', '.', 'It', 'is', 'the', 'story', 'of', 'a', 'person', 'who', 'is', 'cruel', 'and', 'merciless', ',', 'who', 'ultimately', 'comes', 'to', 'realize', 'her', 'own', 'humanity', 'and', 'her', 'own', 'personal', 'power', '.', 'That', ',', 'to', 'me', 'is', 'extremely', 'inspiring', '.', 'If', 'there', 'is', 'hope', 'for', 'Nikita', ',', 'there', 'is', 'hope', 'for', 'all', 'of', 'us', '.']\n",
            "['when', 'i', 'say', 'this', 'is', 'my', 'favourite', 'film', 'of', 'all', 'time', 'that', 'comment', 'is', 'not', 'to', 'be', 'taken', 'lightly', 'i', 'probably', 'watch', 'far', 'too', 'many', 'films', 'than', 'is', 'healthy', 'for', 'me', 'and', 'have', 'loved', 'quite', 'a', 'few', 'of', 'them', 'i', 'first', 'saw', 'la', 'femme', 'nikita', 'nearly', 'ten', 'years', 'ago', 'and', 'it', 'still', 'manages', 'to', 'be', 'my', 'absolute', 'favourite', 'why', 'br', 'br', 'this', 'is', 'more', 'than', 'an', 'incredibly', 'stylish', 'and', 'sexy', 'thriller', 'luc', 'besson', \"'s\", 'great', 'flair', 'for', 'impeccable', 'direction', 'fashion', 'and', 'appropriate', 'usage', 'of', 'music', 'makes', 'this', 'a', 'very', 'watchable', 'film', 'but', 'it', 'is', 'anne', 'parillaud', \"'s\", 'perfect', 'rendering', 'of', 'a', 'complex', 'character', 'who', 'transforms', 'from', 'a', 'heartless', 'killer', 'into', 'a', 'compassionate', 'vibrant', 'young', 'woman', 'that', 'makes', 'this', 'film', 'beautiful', 'i', 'ca', \"n't\", 'keep', 'my', 'eyes', 'off', 'of', 'her', 'when', 'she', 'is', 'on', 'screen.', 'br', 'br', 'i', 'have', 'seen', 'several', 'of', 'luc', 'besson', \"'s\", 'films', 'including', 'subway', 'the', 'professional', 'and', 'the', 'irritating', 'fifth', 'element', 'and', 'nikita', 'is', 'without', 'a', 'doubt', 'far', 'superior', 'to', 'any', 'of', 'these', 'although', 'this', 'film', 'has', 'tragic', 'elements', 'it', 'is', 'ultimately', 'extremely', 'hopeful', 'it', 'is', 'the', 'story', 'of', 'a', 'person', 'who', 'is', 'cruel', 'and', 'merciless', 'who', 'ultimately', 'comes', 'to', 'realize', 'her', 'own', 'humanity', 'and', 'her', 'own', 'personal', 'power', 'that', 'to', 'me', 'is', 'extremely', 'inspiring', 'if', 'there', 'is', 'hope', 'for', 'nikita', 'there', 'is', 'hope', 'for', 'all', 'of', 'us']\n"
          ]
        }
      ],
      "source": [
        "print(doc_tokens[1])\n",
        "print(normalized_tokens[1])\n",
        "# problems like <br> <> removed but br kept\n",
        "# limitations like n't, '99, 're, 's will have to stay"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Option 4: Apostrophes aren't lost\n",
        "\n",
        "import re\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "contractions_pattern = re.compile(r\"\\b\\w+'\\w+\\b\")  # matches words like you'll, can't, doesn't etc.\n",
        "\n",
        "def preprocessing_textNormalizing(text):\n",
        "    # Lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Step 2: Temporarily protect contractions → you'll → you_ll\n",
        "    protected = contractions_pattern.findall(text)\n",
        "    for word in protected:\n",
        "        text = text.replace(word, word.replace(\"'\", \"_\"))  # mark with underscore instead\n",
        "\n",
        "    # Step 3: Remove ALL punctuation except underscores (so apostrophe is now safe)\n",
        "    text = re.sub(r\"[^a-z0-9\\s_]\", \"\", text)\n",
        "\n",
        "    return text\n",
        "\n",
        "def preprocess_textTokenization(text):\n",
        "    # Tokenize into words\n",
        "    tokens = word_tokenize(text)\n",
        "    # Step 4: Restore apostrophes → you_ll → you'll\n",
        "    tokens = [t.replace(\"_\", \"'\") for t in tokens]\n",
        "    # Optionally remove empty tokens\n",
        "    tokens = [t for t in tokens if len(t) > 0]\n",
        "    return tokens\n",
        "\n",
        "unsupervised_texts = [preprocessing_textNormalizing(t) for t in unsupervised_texts]\n",
        "# Example: Apply to your dataset\n",
        "processed_texts = [preprocess_textTokenization(t) for t in unsupervised_texts]\n",
        "print(\", \".join(processed_texts[0]))\n",
        "print(processed_texts[0][0])"
      ],
      "metadata": {
        "id": "vrTych246DS3"
      },
      "id": "vrTych246DS3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "3495790e",
      "metadata": {
        "id": "3495790e"
      },
      "source": [
        "## Step 2: Constructing N-Gram Model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "98c61cc8",
      "metadata": {
        "id": "98c61cc8"
      },
      "source": [
        "### Unigram Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "78ac2f4a",
      "metadata": {
        "id": "78ac2f4a",
        "outputId": "2da71dec-8c04-4c5b-b06b-347e46e7a842"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "12060456\n"
          ]
        }
      ],
      "source": [
        "# Assuming greedy generation\n",
        "# P(w) = count(w)/count(corpus)\n",
        "\n",
        "count_corpus = sum(len(document_tokens) for document_tokens in normalized_tokens)\n",
        "print(count_corpus)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0642ccf4",
      "metadata": {
        "id": "0642ccf4"
      },
      "outputs": [],
      "source": [
        "# count w\n",
        "def count_word(word_to_count):\n",
        "    return sum(1 for document_token in normalized_tokens for word in document_token if word == word_to_count)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e67c4e7",
      "metadata": {
        "id": "8e67c4e7"
      },
      "outputs": [],
      "source": [
        "# need to look through all words and get the maximum.\n",
        "# need to get a list of the vocabulary: unique words\n",
        "vocabulary = set(word for document_tokens in normalized_tokens for word in document_tokens) # Make a set of the flattened words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "edb3097c",
      "metadata": {
        "id": "edb3097c",
        "outputId": "bf5bc5a1-9057-4530-8687-e9567d4aa977"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "166753\n"
          ]
        }
      ],
      "source": [
        "print(len(vocabulary))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a4317783",
      "metadata": {
        "id": "a4317783",
        "outputId": "bb1fede1-caf4-4fa9-badd-4cdb7d747b94"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[85]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      6\u001b[39m sum_ = \u001b[32m0\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m vocabulary:\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m     count_uni_dict[word] = \u001b[43mcount_word\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m     sum_ += count_uni_dict[word]\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(sum_ == count_corpus)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[79]\u001b[39m\u001b[32m, line 3\u001b[39m, in \u001b[36mcount_word\u001b[39m\u001b[34m(word_to_count)\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcount_word\u001b[39m(word_to_count):\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msum\u001b[39m(\u001b[32m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m document_token \u001b[38;5;129;01min\u001b[39;00m normalized_tokens \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m document_token \u001b[38;5;28;01mif\u001b[39;00m word == word_to_count)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[79]\u001b[39m\u001b[32m, line 3\u001b[39m, in \u001b[36m<genexpr>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcount_word\u001b[39m(word_to_count):\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msum\u001b[39m(\u001b[32m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m document_token \u001b[38;5;129;01min\u001b[39;00m normalized_tokens \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m document_token \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mword\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[43mword_to_count\u001b[49m)\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "# Assuming greedy generation\n",
        "# P(w) = count(w)/count(corpus)\n",
        "# we can get most frequent --- like getting highest probability -- to avoid floats\n",
        "# for now\n",
        "count_uni_dict = {}\n",
        "sum_ = 0\n",
        "for word in vocabulary:\n",
        "    count_uni_dict[word] = count_word(word)\n",
        "    sum_ += count_uni_dict[word]\n",
        "print(sum_ == count_corpus)\n",
        "\n",
        "# this code took more than 2 hrs and didn't even run.\n",
        "# I am obviously doing something wrong here and my best guess is the counting method is really bad (linear counting per word)\n",
        "# There might be some other improvements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "c735cd46",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c735cd46",
        "outputId": "d0c6edc5-2a6a-415f-a2af-a41cd5ade919"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hendo\n"
          ]
        }
      ],
      "source": [
        "from collections import Counter\n",
        "\n",
        "# Flatten all tokens from all sentences\n",
        "all_tokens = [word for sentence in processed_texts for word in sentence]\n",
        "\n",
        "# Count how many times each word appears\n",
        "unigram_counts = Counter(all_tokens)\n",
        "# print(unigram_counts)\n",
        "\n",
        "V = len(unigram_counts)  # vocabulary size\n",
        "\n",
        "# Total number of words\n",
        "total_words = sum(unigram_counts.values())\n",
        "# print(total_words)\n",
        "\n",
        "# Compute probability of each word\n",
        "unigram_prob = {word: (count + 1) / (total_words + V) for word, count in unigram_counts.items()}\n",
        "# print(unigram_prob)\n",
        "\n",
        "# Show top 10 (Example)\n",
        "for word, prob in list(unigram_prob.items())[:10]:\n",
        "    print(f\"{word}: {prob:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bigram Model"
      ],
      "metadata": {
        "id": "9LGaU3sV6U0B"
      },
      "id": "9LGaU3sV6U0B"
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict, Counter\n",
        "\n",
        "# Step 1: Count bigrams\n",
        "bigram_counts = defaultdict(Counter)\n",
        "unigram_counts = Counter()\n",
        "\n",
        "for sentence in processed_texts:\n",
        "    for i in range(len(sentence) - 1): #stop 1 word before the end\n",
        "        w1, w2 = sentence[i], sentence[i + 1]\n",
        "        bigram_counts[w1][w2] += 1\n",
        "        unigram_counts[w1] += 1\n",
        "\n",
        "# Laplace smoothing\n",
        "V = len(unigram_counts)  # vocabulary size\n",
        "# Step 2: Compute probabilities\n",
        "bigram_prob = {}\n",
        "\n",
        "for w1, next_words in bigram_counts.items():\n",
        "    total_count = sum(next_words.values())\n",
        "    bigram_prob[w1] = {}\n",
        "    for w2 in unigram_counts:  # iterate over all possible next words\n",
        "        count = bigram_counts[w1][w2]\n",
        "        bigram_prob[w1][w2] = (count + 1) / (unigram_counts[w1] + V)\n",
        "\n",
        "# Show top next words for \"this\" (Example)\n",
        "print(\"Next words after 'i':\")\n",
        "print(bigram_prob.get(\"i\", {}))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WYdqhEqW2MqU",
        "outputId": "5c722514-a73f-4d00-ccd4-ad1d5874ef6e"
      },
      "id": "WYdqhEqW2MqU",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: not a git repository (or any of the parent directories): .git\n",
            "fatal: not a git repository (or any of the parent directories): .git\n",
            "fatal: not a git repository (or any of the parent directories): .git\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Trigram Model"
      ],
      "metadata": {
        "id": "XghtOwq16abf"
      },
      "id": "XghtOwq16abf"
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict, Counter\n",
        "\n",
        "# Step 1: Count trigrams\n",
        "trigram_counts = defaultdict(Counter)   # maps (w1, w2) -> next words and their counts\n",
        "bigram_counts = Counter()               # counts how many times each (w1, w2) pair occurs\n",
        "\n",
        "for sentence in processed_texts:\n",
        "    for i in range(len(sentence) - 2):  # stop 2 words before the end\n",
        "        w1, w2, w3 = sentence[i], sentence[i+1], sentence[i+2]\n",
        "\n",
        "        trigram_counts[(w1, w2)][w3] += 1   # count how many times w3 follows (w1, w2)\n",
        "        bigram_counts[(w1, w2)] += 1        # count how many times (w1, w2) occurs\n",
        "\n",
        "V = len(unigram_counts)  # vocabulary size\n",
        "# Step 2: Compute probabilities\n",
        "trigram_prob = {}\n",
        "\n",
        "for (w1, w2), next_words in trigram_counts.items():\n",
        "    total = bigram_counts[(w1, w2)]\n",
        "    trigram_prob[(w1, w2)] = {}\n",
        "    for w3 in unigram_counts:\n",
        "        count = trigram_counts[(w1, w2)][w3]\n",
        "        trigram_prob[(w1, w2)][w3] = (count + 1) / (total + V)\n",
        "\n",
        "# Example: show what usually comes after (\"i\", \"love\")\n",
        "print(\"Next words after ('i', 'love'):\")\n",
        "print(trigram_prob.get((\"i\", \"love\"), {}))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HIf-HKO32tVI",
        "outputId": "e72c75eb-fc01-415d-a356-56fc6410be8d"
      },
      "id": "HIf-HKO32tVI",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 3: Evaluation Using Perplexity"
      ],
      "metadata": {
        "id": "d8LCrN3-6fFr"
      },
      "id": "d8LCrN3-6fFr"
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "from collections import defaultdict, Counter\n",
        "\n",
        "def calculate_perplexity(test_texts, ngram_prob, n_minus1_counts, V, n):\n",
        "    total_log_prob = 0\n",
        "    total_words = 0\n",
        "\n",
        "    for sentence in test_texts:\n",
        "        if len(sentence) < n:\n",
        "            continue\n",
        "\n",
        "        for i in range(len(sentence) - n + 1):\n",
        "            ngram = tuple(sentence[i:i + n])\n",
        "            context = ngram[:-1]\n",
        "            word = ngram[-1]\n",
        "\n",
        "            total_words += 1\n",
        "\n",
        "            # Get probability (handle unseen)\n",
        "            if context in ngram_prob and word in ngram_prob[context]:\n",
        "                prob = ngram_prob[context][word]\n",
        "            else:\n",
        "                context_count = n_minus1_counts[context] if context in n_minus1_counts else 0\n",
        "                prob = 1 / (context_count + V)\n",
        "\n",
        "            total_log_prob += math.log(prob)\n",
        "\n",
        "    perplexity = math.exp(-total_log_prob / total_words)\n",
        "    return perplexity"
      ],
      "metadata": {
        "id": "d2xX12Gx6jxC"
      },
      "id": "d2xX12Gx6jxC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comparing the Three Models"
      ],
      "metadata": {
        "id": "5wyCmAyO6mZB"
      },
      "id": "5wyCmAyO6mZB"
    },
    {
      "cell_type": "code",
      "source": [
        "pp_uni = calculate_perplexity(IMDB_Dataset['test'], unigram_prob, Counter({(): sum(unigram_counts.values())}), V, 1)\n",
        "pp_bi  = calculate_perplexity(IMDB_Dataset['test'], bigram_prob, unigram_counts, V, 2)\n",
        "pp_tri = calculate_perplexity(IMDB_Dataset['test'], trigram_prob, bigram_counts, V, 3)\n",
        "\n",
        "print(f\"Unigram Perplexity: {pp_uni:.2f}\")\n",
        "print(f\"Bigram  Perplexity: {pp_bi:.2f}\")\n",
        "print(f\"Trigram Perplexity: {pp_tri:.2f}\")"
      ],
      "metadata": {
        "id": "BaAmETt-6o1S"
      },
      "id": "BaAmETt-6o1S",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "nlp_env_py3118",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}